version: "3"

services:
  tritoninferenceserver:
    build:
      context: .
      dockerfile: ./server/Dockerfile
    container_name: BoonMoSa_TritonInferenceServer
    volumes:
      - /home/ec2-user/Amazon-EC2-Inf1/server/model-repository:/model-repository
    command: tritonserver --model-repository=/model-repository --strict-model-config=false --log-verbose=1 --backend-config=python,grpc-timeout-milliseconds=50000
    ports:
      - 8000:8000
      - 8001:8001
      - 8002:8002
    shm_size: 4gb
    pid: host

  fastapi:
    build:
      context: .
      dockerfile: ./client/Dockerfile
    container_name: BoonMoSa_FastAPI
    volumes:
      - /home/ec2-user/Amazon-EC2-Inf1/client/inputs:/app/inputs
      - /home/ec2-user/Amazon-EC2-Inf1/client/outputs:/app/outputs
    ports:
      - 80:80

networks:
  default:
    name: BoonMoSa-network