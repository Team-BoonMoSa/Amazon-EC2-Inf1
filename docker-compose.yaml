version: "3"

services:
  tritoninferenceserver:
    build:
      context: .
      dockerfile: ./server/Dockerfile
    container_name: BoonMoSa_TritonInferenceServer
    devices:
      - /dev/neuron0
    volumes:
      - /home/ubuntu/python_backend:/home/ubuntu/python_backend
      - /lib/udev:/mylib/udev
      - /home/ubuntu/Amazon-EC2-Inf1/server/model-repository:/model-repository
    shm_size: 8g
    ports:
      - 8000:8000
      - 8001:8001
      - 8002:8002
    command: tritonserver --model-repository=/model-repository

  fastapi:
    build:
      context: .
      dockerfile: ./client/Dockerfile
    container_name: BoonMoSa_FastAPI
    volumes:
      - /home/ubuntu/Amazon-EC2-Inf1/client/inputs:/app/inputs
      - /home/ubuntu/Amazon-EC2-Inf1/client/outputs:/app/outputs
    ports:
      - 80:80

networks:
  default:
    name: BoonMoSa-network